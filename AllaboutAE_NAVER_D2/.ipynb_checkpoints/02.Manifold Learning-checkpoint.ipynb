{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img16](./img/img16.png)\n",
    "- Manifold란 트레이닝 DB 데이터 전체의 각각의 샘플을 데이터 공간에서 봤을때 각각의 데이터를 오차없이 잘 아우르는 어떤 subspace를 의미한다.\n",
    "    - 원 데이터 공간안에 속해있는(원 데이터 공간보다 저차원) 어떤 subspace\n",
    "- 이 subspace를 잘 찾으면 이걸 projection(사영)해서 차원 축소를 할 수 있을 것이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction 관점에서 Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Compression(데이터 압축)\n",
    "1. Data Visualization(시각화)\n",
    "1. Curse of Dimensionality(차원의 저주 해결)\n",
    "1. Discovering most important features(유용한 feature 발견)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img17](./img/img17.png)\n",
    "- 데이터의 차원이 증가할수록 해당 데이터 공간의 크기가 기하급수적으로 증가하기 때문에 동일한 개수의 데이터 밀도는 급속도로 낮아진다.\n",
    "- 따라서 차원이 증가할수록 모델링에 필요한 데이터 샘플의 개수가 기하급수적으로 증가하게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manifold Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Natural data in high dimensional spaces concentrates close to lower dimensional manifolds.\n",
    "    - 고차원 데이터를 원 공간에서 봤을때 데이터들이 잘 밀집되어 있는 어떤 manifold가 존재할 것이다.\n",
    "1. Probability density decreases very rapidly when moving away from the supporting manifold.\n",
    "    - 그 manifold를 벗어나면 데이터 밀도는 급격히 낮아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과연 Manifold 가정이 합당한 가정인가?\n",
    "- 200X200 RGB 이미지(고차원 데이터)에서 uniform sampling 했을 때 샘플링 된 이미지를 보면 noisy한 이미지 밖에 나오지 않는다.\n",
    "- 즉, 유의미한 이미지 데이터는 원 데이터 공간에 균일하게 퍼져있지 않고 어느 한쪽에 몰려있다고 생각할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discovering most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img18](./img/img18.png)\n",
    "- 만약에 데이터 압축이 잘 됐다면, 즉 manifold를 잘 찾았다면 feature를 잘 찾았다고 할 수 있다.\n",
    "- 압축을 했더니 자동적으로 dominant feature가 찾아지는것 -> manifold를 잘 찾은것. 그런데 왜 자동인가? Unsupervised Learning 이기때문에."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img19](./img/img19.png)\n",
    "- 원 데이터 공간(고차원 공간)에서 의미적으로 가깝게 보이는 것들이 실제로는 그렇지 않을 수 있다.\n",
    "- 위 그림에서 고차원 공간에서는 B와 A1이 더 가까워 보이지만 실제로 manifold에서는 A2가 B와 더 가깝다는 것을 볼 수 있다.\n",
    "- 즉, 고차원 공간에서 가까운 두 샘플들은 의미적으로 굉장히 다를 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img20](./img/img20.png)\n",
    "manifold를 흔히 두 가지로 표현한다.\n",
    "- Entangled manifold : dominant feature를 잘 찾아내지 못한것. 학습이 잘 되지않은것.\n",
    "- Disentangled manifold : dominant feature에 따라 잘 학습된 것. 위에 MNIST데이터 예시를 보면 같은 숫자들끼리 뭉쳐있는것을 볼 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear : PCA, LDA,...\n",
    "- Non-linear : AE, t-SNE, Isomap, LLE,...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Estimation\n",
    "- Parzen Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 위에서 나열한 기존의 Dimensionality Reduction, Density Estimation 방법론들은 모두  ***Neighborhood based training!!***\n",
    "- Neighborhood based training은 유클리디안 거리 개념을 이용하는데 고차원공간에서는 아까 말했다시피 유클리디안 거리는 유용한 개념이 아닐 가능성이 높다.\n",
    "- AE는 Neighborhood based training이 아니다!! 고차원 데이터일수록 데이터가 많을수록 AE가 유리하다. 하지만 얼마나 고차원이고 얼마나 많은 데이터일지는 경험적으로 얻을 수 밖에 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lecture : https://www.youtube.com/watch?v=o_peo6U7IRM&t=2545s (오토인코더의 모든것-1/3)\n",
    "- slide : https://www.slideshare.net/NaverEngineering/ss-96581209"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
