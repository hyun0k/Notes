{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03.Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img21](./img/img21.png)\n",
    "- 입출력이 동일한 네트워크\n",
    "- Unsupervised Learning 문제를 Supervised(self) Learning 문제로 바꿔서 풀 수 있었다.\n",
    "- Encoder, Decoder는 최소한 트레이닝 DB 안에 있는 데이터는 잘 압축하고 복원하는 minimum 성능이 보장됨.\n",
    "    - GAN은 minimum 성능 보장이 전혀 없음. 학습이 어렵다.\n",
    "- 하지만, 생성되는 데이터가 학습데이터와 비슷한 성향을 보인다.(완전히 새로운건 만들어지지 않는다.)\n",
    "\n",
    "Linear Autoencoder\n",
    "- Autoencoder에서 히든레이어에 activation function을 사용하지 않는 형태.\n",
    "- mse를 loss function으로 쓰는 linear AE로 찾은 manifold는 PCA로 찾는 manifold와 같다. 단 manifold를 구성하는 basis는 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking AE\n",
    "- AE가 처음 나왔을때 네트워크 parameter를 초기화(Pre-training)하는데에 많이 쓰였다.\n",
    "- 그 당시에는 Batch-Norm, Xavier Initializer도 없었을 때인데 AE로 초기화한 뒤 학습시키면 성능이 좋았다.(DNN에서 초기화는 중요한 issue) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denoising AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img22](./img/img22.png)\n",
    "- 입력에 noise를 추가시키고 loss function은 noise를 추가시키기 전 값과 decoder출력값으로 구성.\n",
    "- noise는 어느정도 추가하는가? 사람이 봤을때 noise를 추가시키기 전과 똑같다고 생각할정도로 추가. 그럼 결국 noise를 추가하기 전과 같은 manifold로 학습된다.\n",
    "- 의미적으로 같으면 같은 manifold상에 있어야한다. 따라서 noise가 추가되어도 의미적으로 원 샘플과 같다고 학습이 돼야 하므로 manifold학습이 더 잘된다. \n",
    "![img23](./img/img23.png)\n",
    "![img24](./img/img24.png)\n",
    "- DAE의 Performance를 어떻게 확인했는가? filter를 시각화해서 보여줌.\n",
    "- 일반적으로 CNN에서 각 layer별로 추출되는 feature 수준이 다르다. 예를 들어 하위 레벨 레이어에서는 low level feature, 예를 들어 이미지의 방향별 Edge 같은 것이 추출되고 다음 레이어에서는 좀 더 상위 레벨의 feature가 추출된다고 알려져 있음.\n",
    "    - 그렇다면 제일 하위 레이어에서는 Edge detector같은 것이 학습이 되어야 학습이 잘 된 것이다.\n",
    "    - 레이어를 하나만 놓고 랜덤값으로 초기화시켰으므로 노이즈처럼 보이는 필터일수록 학습이 잘 안된것이다.\n",
    "    - 학습시킨 결과 DAE로 학습시킨 것이 확실히 Edge detector모양이 잘 나온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Contractive AE\n",
    "- 기존 AE의 loss function에 regularizaion term 이 추가되었다.\n",
    "- manifold상에서 원 샘플과 noise를 추가한 샘플 간 거리가 가깝도록 하는 regularization.\n",
    "- ${L_{SCAE} = \\sum_{x \\in D}} {\\color{Blue}{L(x,g(h(x)))}} + {\\lambda} {\\color{Red}{E_{q(\\tilde{x} | x)}\\left[||h(x)-h(\\tilde{x})||^{2}\\right]}}  $\n",
    "- CAE는 위에서 regularization term을 taylor expansion을 통해 deterministic하게 바꾼 것.\n",
    "- ${\\color{Red}{E_{q(\\tilde{x} | x)}\\left[||h(x)-h(\\tilde{x})||^{2}\\right]}}{\\approx}{\\color{Blue}{\\displaystyle{\\left\\|\\frac{\\partial{h}}{\\partial{x}}(x)\\right\\|}^{2}_{F}}}$\n",
    "- 하지만 CAE는 잘 안쓴다. AE, DAE, VAE 이 세가지만 쓴다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lecture : https://www.youtube.com/watch?v=o_peo6U7IRM&t=2546s (오토인코더의 모든것-1/3)\n",
    "- slide : https://www.slideshare.net/NaverEngineering/ss-96581209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
